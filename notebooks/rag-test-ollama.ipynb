{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35b1666-5393-458a-b747-177c1a1525ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'meal_type', 'dietary_category', 'ingredients',\n",
    "       'equipment_needed', 'nutritional_information', 'instructions', 'allergens', 'prep_time_minutes'],\n",
    "    keyword_fields=['id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all fields in text_fields are strings\n",
    "for doc in documents:\n",
    "    for field in ['recipe_name', 'meal_type', 'dietary_category', 'ingredients',\n",
    "                  'equipment_needed', 'nutritional_information', 'instructions', 'allergens', 'prep_time_minutes']:\n",
    "        doc[field] = str(doc[field]) if doc[field] is not None else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1488243d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Setup & LLM Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def ollama_generate(model, prompt, temperature=0.0):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature,\n",
    "        \"options\": {\n",
    "            \"num_ctx\": 8192\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ollama error: {response.text}\")\n",
    "    \n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Flow (Now using Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {\n",
    "        'recipe_name': 3.0,\n",
    "        'meal_type': 1.5,\n",
    "        'dietary_category': 1.0,\n",
    "        'ingredients': 3.5,\n",
    "        'equipment_needed': 1.0,\n",
    "        'nutritional_information': 2.0,\n",
    "        'instructions': 1.0,\n",
    "        'allergens': 1.5,\n",
    "        'prep_time_minutes': 0.5\n",
    "    }\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a nutrition assistant named BiteBlender. Answer the QUESTION based on the CONTEXT from our nutrition database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "entry_template = \"\"\"\n",
    "recipe_name: {recipe_name}\n",
    "meal_type: {meal_type}\n",
    "dietary_category: {dietary_category}\n",
    "ingredients: {ingredients}\n",
    "equipment_needed: {equipment_needed}\n",
    "nutritional_information: {nutritional_information}\n",
    "instructions: {instructions}\n",
    "allergens: {allergens}\n",
    "prep_time_minutes: {prep_time_minutes}\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context += entry_template.format(**doc) + \"\\n\\n\"\n",
    "    \n",
    "    return prompt_template.format(question=query, context=context).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model=\"qwen2:1.5b\"):\n",
    "    return ollama_generate(model=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model=\"qwen2:1.5b\"):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2:1.5b:\n",
      "The ingredients needed for Lentil Soup are as follows:\n",
      "\n",
      "- Cucumber\n",
      "- Mushroom\n",
      "- Quinoa (if not using gluten-free options)\n",
      "- Salmon (or chicken)\n",
      "- Avocado\n",
      "- Bell Pepper (optional)\n",
      "- Spinach or kale (for garnish if serving it on the side)\n",
      "- Onion (if desired for flavor, not necessary for soup itself)\n",
      "- Tomatoes (if you want to add acidity and tomato-based taste, optional but recommended)\n",
      "- Grated cheese (for adding creaminess, not essential for soup alone)\n",
      "\n",
      "Please note that the ingredients may vary based on personal preferences or specific diet requirements.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Llama3.2:3b:\n",
      "Based on the provided CONTEXT, I can see that there are two recipes for Lentil Soup.\n",
      "\n",
      "The recipe \"Lentil Soup\" (meal_type: Breakfast, dietary_category: Gluten-Free) has the following ingredients:\n",
      "\n",
      "1. quinoa\n",
      "2. mushroom\n",
      "3. chicken\n",
      "\n",
      "These are the ingredients needed for this specific recipe.\n"
     ]
    }
   ],
   "source": [
    "question = \"What ingredients are needed for lentil soup?\"\n",
    "\n",
    "print(\"Qwen2:1.5b:\")\n",
    "print(rag(question, model=\"qwen2:1.5b\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Llama3.2:3b:\")\n",
    "print(rag(question, model=\"llama3.2:3b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation (Same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question = pd.read_csv('ground-truth-retrieval.csv')\n",
    "ground_truth = df_question.to_dict(orient='records')\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    return sum(any(line) for line in relevance_total) / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank, val in enumerate(line):\n",
    "            if val:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    return {'hit_rate': hit_rate(relevance_total), 'mrr': mrr(relevance_total)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97464f0191fb4f79b2b9b253dbaa9c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.26772616136919314, 'mrr': 0.09609917724220919}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: search(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation with Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def evaluate_rag(model_name, n_samples=200):\n",
    "    sample = df_question.sample(n=n_samples, random_state=1).to_dict(orient='records')\n",
    "    evaluations = []\n",
    "\n",
    "    for record in tqdm(sample, desc=f\"Evaluating {model_name}\"):\n",
    "        q = record['question']\n",
    "        answer_llm = rag(q, model=model_name)\n",
    "\n",
    "        eval_prompt = prompt2_template.format(question=q, answer_llm=answer_llm)\n",
    "        evaluation = ollama_generate(model=\"llama3.2:3b\", prompt=eval_prompt)\n",
    "        \n",
    "        try:\n",
    "            evaluation_json = json.loads(evaluation)\n",
    "        except:\n",
    "            evaluation_json = {\"Relevance\": \"NON_RELEVANT\", \"Explanation\": \"Failed to parse evaluator response\"}\n",
    "        \n",
    "        evaluations.append((record, answer_llm, evaluation_json))\n",
    "\n",
    "    df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "    df_eval['id'] = df_eval.record.apply(lambda x: x['id'])\n",
    "    df_eval['question'] = df_eval.record.apply(lambda x: x['question'])\n",
    "    df_eval['relevance'] = df_eval.evaluation.apply(lambda x: x.get('Relevance', 'NON_RELEVANT'))\n",
    "    df_eval['explanation'] = df_eval.evaluation.apply(lambda x: x.get('Explanation', ''))\n",
    "    \n",
    "    del df_eval['record']\n",
    "    del df_eval['evaluation']\n",
    "    \n",
    "    print(f\"\\n=== Results for {model_name} ===\")\n",
    "    print(df_eval.relevance.value_counts(normalize=True))\n",
    "    \n",
    "    df_eval.to_csv(f'../data/rag-eval-{model_name.replace(\":\", \"-\")}.csv', index=False)\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd9e01d433f44e99629291232d2a988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating qwen2:1.5b:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for qwen2:1.5b ===\n",
      "relevance\n",
      "NON_RELEVANT       0.926667\n",
      "PARTLY_RELEVANT    0.053333\n",
      "RELEVANT           0.020000\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda9abd8ccba44df817e03728e44f60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating llama3.2:3b:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for llama3.2:3b ===\n",
      "relevance\n",
      "NON_RELEVANT       0.933333\n",
      "PARTLY_RELEVANT    0.046667\n",
      "RELEVANT           0.020000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for both models\n",
    "eval_qwen = evaluate_rag(\"qwen2:1.5b\", n_samples=150)\n",
    "eval_llama = evaluate_rag(\"llama3.2:3b\", n_samples=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
